cluster_name: ray-llm-finetuning-demo-gpu-cluster

max_workers: 2

upscaling_speed: 1.0

docker:
    image: "rayproject/ray-ml:2.7.1.artur.097c30-py310-gpu" #"rayproject/ray-ml:latest-gpu"
    container_name: "ray_nvidia_docker"

idle_timeout_minutes: 5

provider:
    type: aws
    region: us-west-2
    availability_zone: us-west-2a
    cache_stopped_nodes: False 
    cloudwatch:
        agent:
            config: "cloudwatch/cloudwatch-agent-config.json"
        dashboard:
            name: "RayDashboard"
            config: "cloudwatch/cloudwatch-dashboard-config.json"

auth:
    ssh_user:  ubuntu #ec2-user

available_node_types:
    ray.head.default:
        resources: {}
        node_config:
            InstanceType: r5dn.xlarge #r5dn.4xlarge 
            ImageId: ami-0a2363a9cff180a64 # us-west-2 DL AMI Ubuntu Version 30
            BlockDeviceMappings:
                - DeviceName: /dev/sda1
                  Ebs:
                      VolumeSize: 500

    ray.worker.default:
        docker:
            worker_image: "rayproject/ray-ml:2.7.1.artur.097c30-py310-gpu" #"rayproject/ray-ml:latest-gpu"
        min_workers: 2 
        max_workers: 2
        resources: {}
        node_config:
            InstanceType: p2.xlarge
            ImageId: ami-0a2363a9cff180a64 # us-west-2 DL AMI Ubuntu Version 30
            BlockDeviceMappings:
                - DeviceName: /dev/sda1
                  Ebs:
                      VolumeSize: 500


file_mounts: {
    "~/infra": "./infra"
}

head_node_type: ray.head.default

setup_commands:
    - pip install -U pip 
    - pip install -r infra/requirements.txt

head_start_ray_commands:
    - ulimit -n 65536
    - ray stop
    - ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --dashboard-host=0.0.0.0
    - python -c "import ray; print(ray.__version__)"
    - python -c "import sys; print(sys.version)"

worker_start_ray_commands:
    - ulimit -n 65536
    - ray stop
    - ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076
    - python -c "import ray; print(ray.__version__)"
    - python -c "import sys; print(sys.version)"

initialization_commands: []

head_setup_commands:
    - pip install -U pip
    - pip install -r infra/requirements.txt

worker_setup_commands:
    - pip install -U pip
    - pip install -r infra/requirements.txt

    # head_node_type: ray.head.default

# setup_commands: 
#     - pip install -U torch 
#     - conda install -y cudatoolkit

# head_setup_commands:
#     - pip install -U jupyterlab mlflow boto3 
#     - nohup mlflow ui --host 0.0.0.0 --port 5001 > mlflow.out &
#     - nohup jupyter lab > jupyterlab.out &

# worker_setup_commands: []

# head_start_ray_commands:
#     - ray stop
#     - export AUTOSCALER_MAX_NUM_FAILURES=inf; ulimit -n 65536; ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml

# worker_start_ray_commands:
#     - ray stop
#     - ulimit -n 65536; ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076

